Natural Language Processing - Evaluation Metrics

BLEU, or the Bilingual Evaluation Understudy, is a score for comparing a
candidate translation of text to one or more reference translations. Although
developed for translation, it can be used to evaluate text generated for
a suite of natural language processing tasks.

ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of
metrics and a software package used for evaluating automatic summarization and
machine translation software in natural language processing. The metrics compare
an automatically produced summary or translation against a reference or a set
of references (human-produced) summary or translation.

In information theory, perplexity is a measurement of how well a probability
distribution or probability model predicts a sample. It may be used to compare
probability models. A low perplexity indicates the probability distribution
is good at predicting the sample.
