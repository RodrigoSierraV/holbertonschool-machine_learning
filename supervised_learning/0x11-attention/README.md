Attention is all you need:

    The attention mechanism is one of the most valuable breakthroughs in
    Deep Learning research in the last decade. It has spawned the rise of
    so many recent breakthroughs in natural language processing (NLP),
    including the Transformer architecture and Googleâ€™s BERT
    
    The first type of Attention, commonly referred to as Additive Attention,
    came from a paper by Dzmitry Bahdanau, which explains the less-descriptive
    original name. The paper aimed to improve the sequence-to-sequence
    model in machine translation by aligning the decoder with the relevant
    input sentences and implementing Attention.
    
    The second type of Attention was proposed by Thang Luong in this paper.
    It is often referred to as Multiplicative Attention and was built on
    top of the Attention mechanism proposed by Bahdanau. The two main
    differences between Luong Attention and Bahdanau Attention are:
    
        The way that the alignment score is calculated
        The position at which the Attention mechanism is being introduced in the decoder
