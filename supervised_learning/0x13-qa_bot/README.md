What is question-answering?

In Question Answering tasks, the model receives a question regarding text
content and is required to mark the beginning and end of the answer in the text.

BERT is a trained Transformer Encoder stack, with twelve in the Base version,
and twenty-four in the Large version. BERT was trained on Wikipedia and Book
Corpus, a dataset containing +10,000 books of different genres.

Machine reading comprehension has captured the minds of computer scientists
for decades. The recent production of large-scale labeled datasets has allowed
researchers to build supervised neural systems that automatically answer
questions posed in a natural language.
